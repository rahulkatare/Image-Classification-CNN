{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "base_address = \"output/\"\n",
    "def getImageData(im):\n",
    "    rgbIm = im.convert(\"RGB\")\n",
    "    width = rgbIm.width\n",
    "    height = rgbIm.height\n",
    "    img = []\n",
    "    for i in range(height):\n",
    "        row = []\n",
    "        for j in range(width):\n",
    "            row.append(list(rgbIm.getpixel((i,j))))\n",
    "        img.append(row)\n",
    "    return img\n",
    "\n",
    "def getData(base_address):\n",
    "    allFiles = os.listdir(base_address)\n",
    "    X = []\n",
    "    Y = []\n",
    "    classes={}\n",
    "    for file in allFiles:\n",
    "        im = Image.open(base_address+file)\n",
    "        X.append(getImageData(im))\n",
    "        class_x = int(file.split('_')[5].split('.')[0])\n",
    "        classes[class_x-1]=1\n",
    "        Y.append(class_x-1)\n",
    "    return np.array(X), np.array(Y), list(classes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 28, 2, 13, 10, 18, 7, 45, 15, 22, 53, 48, 1, 44, 4, 23, 40, 34, 51, 9, 6, 3, 5, 0, 54, 24, 8, 27, 58, 46, 55, 30, 19, 37, 21, 32, 35, 59, 57, 43, 33, 29, 52, 41, 25, 17, 39, 14, 49, 31, 36, 20, 38, 26, 12, 56, 11, 42, 16, 47]\n"
     ]
    }
   ],
   "source": [
    "X,Y, classes = getData(base_address)\n",
    "X_train_orig, X_test_orig, y_train, y_test = train_test_split(X,Y, test_size=0.2,random_state=42)\n",
    "Y_train_orig = np.array([y_train])\n",
    "Y_test_orig = np.array([y_test])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAZvklEQVR4nO2df2xc1ZXHvweHkF/g2IkJxk5J0gRKCuQHFpBCIQk/ytLQqgihIroCFCnSqrtK1W5buiut2tWuRLXStpX2FxHtFqndBtqmmzSgtkkWWtGSlASSlPww+UF+2LXjJMT5RZPg9Owf83xz7mXGnsy8eTP2/X4ka8579817Zzw+vufcc++5oqoghAx/Lqm2AoSQbKCxExIJNHZCIoHGTkgk0NgJiQQaOyGRUJaxi8j9ItIuIrtF5Km0lCKEpI+UmmcXkToAbwO4F0AHgNcBPKqq29NTjxCSFiPKeO8tAHar6l4AEJHlAD4NoKCxiwhn8GTIiBGFv96+vr4MNSFZoqqS73w5xt4C4KA57gBwaxn3IynT0NDg5NCDe/fdd5385z//2WsTyfu38oF7kKFFOcZeFCKyBMCSSj+HEDIw5Rh7J4DJ5rg1OeehqssALAPoxleCMWPGeMdLly518pkzZ5y8fPly77qwN7ewBx+elDMa/zqAGSIyVURGAvgsgFXpqEUISZuSe3ZV7RORvwbwSwB1AL6nqttS04wQkiplxeyq+hKAl1LShRBSQSo+QEfSZ/LkC0Mljz32mNfW3t7u5BUrVhR1v3D0nTH78ITTZQmJBBo7IZFAN77CpOEi19fXe8cPPPCAk0+fPu21vfTSxQ+h0G2PA/bshEQCjZ2QSKCxExIJJS9xLelhnC7rxfAD/e6vvvpqJz/++ONeW1dXl5N/8IMfeG12NVuxzyLDi0Kr3tizExIJNHZCIoGpt4wZyJ2ePn26kz/3uc85ef369d51a9eudfJARSjouhMLe3ZCIoHGTkgk0I3PmAkTJjj5wQcf9No++tGPOvmFF15w8qZNm7zrbOGJuro6r+38+fNF6cGR+vhgz05IJNDYCYkEGjshkcAZdCVSbMxrV6gBwIIFC5z8u9/9zmtbuXKlkwcqCFksra2tTr7xxhu9tiuvvNLJx44d89r27dvn5J6eHifb8tMAcO7cubJ1JOnDGXSERA6NnZBIoBtfAR566CEnh4Un1q1b5+QDBw54bTaNNn78+IL3GDt2rJMbGxu9tmuuucbJl19+uZOPHDniXdfb25v3WQDQ3Nyc97pXXnnFu866+6R2oBtPSOTQ2AmJBBo7IZHA6bIl0tbW5uQnn3zSa/vTn/7kZJtOA4CrrrrKyfPmzfPampqanGzjaBt7A37Ky8bUAHDw4IWNdV977TUn79q1K8+nyBHWnv/Qhz7k5K1btzr50KFDBe9Bap9Be3YR+Z6I9IjIW+Zco4isEZFdyWvDQPcghFSfYtz47wO4Pzj3FIB1qjoDwLrkmBBSwxSVehORKQBWq+oNyXE7gPmq2iUizQBeUdXrirjPkE69LVq0yMlf/vKXnRy60s8884yTbZoM8GfGWXcf8GeodXZe2P3azmIDgPfff7+gjvb7tHKYXrMz+66//nqvbfPmzU62qcLwc5LaJO3U2yRV7a962A1gUon3IYRkRNkDdKqqA/XYIrIEwJJyn0MIKQ+68QNw2WWXece2vLN1x8NR6jNnzhS8h3XB01jsMhA2hHj44Ye9tlmzZjn5N7/5jde2evVqJw9U447UJmm78asA9BczfxzAygGuJYTUAMWk3n4E4DUA14lIh4gsBvA0gHtFZBeAe5JjQkgNM2jMrqqPFmi6O2VdCCEVhKvehjFPPPGEk+fOneu1/frXv3ZyuM1zmBIkQwuueiMkcmjshEQC3fgapdgad3bLKMDf8dWmzV588UXvuo0bN5arIqlR6MYTEjk0dkIigcZOSCSweEUFsPG2lYHCq9JCBmqbP3++kz/zmc94be+8846Tf/7znzt5z549hRUmUcCenZBIoLETEglMvdUoth586Kpfe+21Tl6/fr3XtnbtWiefPHmyQtqRWoapN0Iih8ZOSCTQja8ithgGAMyePdvJLS0tTj579qx33W9/+1snh6PstmCFLUEd7rh66tSpgm1kaEM3npDIobETEgk0dkIiYUjH7OHsNEulP5d9drg9k02b2e2eAGDmzJlOvvfee722GTNmONnOhLMxOgB0dHQUfPbEiROdbGNxW5Me8LeJCreOPnz4sJNtbJ/l3wopHcbshEQOjZ2QSBhWbnylP8vkyZOdfMMNNzjZut+AnzabNMnfLMe62XV1dV6b3fJp//79Tg5TY5deeqmTw62g7FZRtpbcddf5Zf2vueYaJ9vtngBgxYoVTu7q6nLy+fPnQWofuvGERA6NnZBIoLETEglDunhFJWJ0Ow5wxx13eG133XWXk0eOHOnkcD80G+du2rTJa/vjH/+YVwb8eNvG4mFsbwnjaDu1trm52cl2rAAApk2blvc9AHDixAknV3o/OpIdxWz/NFlEXhaR7SKyTUSWJucbRWSNiOxKXhsqry4hpFSKceP7AHxJVWcCuA3A50VkJoCnAKxT1RkA1iXHhJAa5aJTbyKyEsC/JT8XtW1zqam3Ymuol8K4ceO84w9/+MNOfvRRf5u73bt3O/nZZ59NVY+0uOWWW5y8YMECJ4ehwLZt25z86quvem1Hjx6tkHYkC1JJvSX7tM8BsAHAJFXtD067AUwq8DZCSA1Q9ACdiIwD8FMAX1DVE0Fvq4V6bRFZAmBJuYoSQsqjqJ5dRC5FztB/qKr906sOJe47kteefO9V1WWq2qaqbWkoTAgpjUFjdsl14c8BeFdVv2DO/wuAo6r6tIg8BaBRVb8yyL24bCoFbDWaj33sY17bfffd52SbNgtXzr322mtOtqvcBuKSSy4peBymALlCrnoUitmLceNvB/CXAP4gIv2TqP8OwNMAXhCRxQD2A3gkDUUJIZVhUGNX1VcBFFo4fne66hBCKsWQnkE3FEhjZd7NN9/sHX/yk5908pQpU7y2nTt3OnnVqlVOtqvoAH9FXLGEulvXnW577cO58YREAo2dkEgY0sUrhjp2VB0Apk6d6uTbb7/dyTfeeKN3na0nt2XLFq/Njrp3d3eXrWM16/yR0mDxCkIih8ZOSCTQ2AmJBKbeLoI0Vt/ZQo9hcYzrr7/eyU1NTU5+4403vOtWr17tZFsoA0i/2ATj8uEDe3ZCIoHGTkgkMPU2AKXOfpswYYKTbX15AJg3b56TW1tbvTY7+80uVHnzzTe969Jw1StZECSL+5PCMPVGSOTQ2AmJBBo7IZHAmL1E7H5r4bbMH//4x50c7gNn68Fv3brVa7OFH3t7e1PRsxCcBjt8YcxOSOTQ2AmJhCHhxtdKGse66zfddJOTba15AGhouLA5zttvv+212e2g3nnnnbRVJIRuPCGxQ2MnJBKGhBufJbagxMyZM722RYsWOXnOnDlOtruvAr6rvn79eq/N1n4bM2aM12Z3hj1+/LiTw91ebRshIXTjCYkcGjshkUBjJyQSoixeYePycPbbPffc4+SwuERLS4uTz5075+TTp09719nZdWHN90mTLmx229jY6LXZ8RObltu3b593XXt7u5PD8YIzZ87k1ZGQQXt2ERklIr8XkS0isk1EvpGcnyoiG0Rkt4g8LyIjB7sXIaR6FOPGnwWwUFVnAZgN4H4RuQ3ANwF8S1WnAzgGYHHl1CSElMtFpd5EZAyAVwH8FYAXAVylqn0iMg/A11X1E4O8vyZm0E2fPt3Jtj47ACxcuNDJZ8+e9dpsTXa782lY/92m0MIFJ9a1Du9vXXB7T+v6A77739nZ6bXZxTXbt2/Pe28yvCkr9SYidckOrj0A1gDYA6BXVfuSSzoAtBR6PyGk+hRl7Kp6XlVnA2gFcAuAjxT7ABFZIiIbRWRjiToSQlLgolJvqtoL4GUA8wCMF5H+0fxWAJ0F3rNMVdtUta0sTQkhZTFo6k1EmgC8r6q9IjIawL3IDc69DOBhAMsBPA5gZVpKDVRYIQ1snLtmzRqv7fXXX3dyuK3xkSNHnGyLPoYx++jRo/NeBwAnTpwoeH9b2GLixIlODgtTPvTQQ062BSzDe9i0XEdHB0qBRS6GD8Xk2ZsBPCcidch5Ai+o6moR2Q5guYj8E4A3AXy3gnoSQspkUGNX1a0A5uQ5vxe5+J0QMgSoyRl0lXYPrfscutLhCrNiCGfQpYFN7VkZ8FOHoRs/atQoJ48bN65sPeiqDx84N56QSKCxExIJNenGk4Gpq6tz8ogR/ldoMwFhcQwSN+zZCYkEGjshkUBjJyQSGLMPAWxxSwCYNm2ak8MZbqdOnXJymFYkccOenZBIoLETEgl04yuMLWQB+DPcwkUytpjFrbfe6mRbUAPwi1eEW0jt2LHDyd3d3SVo7MOFMMMH9uyERAKNnZBIoLETEgmZ7/VWKAYcTvHf5Zdf7uSw8MTVV1/t5CuvvNJrs1Nf7TbQtg494NeRt8U2AGDXrl1OTmM1Xhoxe61suR0L3OuNkMihsRMSCZm78Zdckvv/Ej43Sz36dQA+uGrssssuyysP9L4wvdbQ0OBkW2gC8N1466oD/u+gq6vLyeG2z9aN379/f8F7pA3TcEMDuvGERA6NnZBIyNyNz+xhH3y2k+vr6/PKgF8W2o6qA75bb99nC0YAvltvC02EhO6/LTO9d+9eJ+/cudO77r333nPy+fPnC96/WEL3vJRR9oGgi58tdOMJiRwaOyGRQGMnJBKG7aq3cNbZFVdc4eSbbrrJybNmzfKus7XW7Qo1wI+P+/r6nHzs2DHvut7eXifv2bPHa3v33XedfPz4ca/NznizhSfCrZ3TIO0ttkqN+0l2FN2zJ9s2vykiq5PjqSKyQUR2i8jzIjJysHsQQqrHxbjxSwHsMMffBPAtVZ0O4BiAxWkqRghJl6JSbyLSCuA5AP8M4IsAHgRwGMBVqtonIvMAfF1VPzHIfVL17cLtjVpaWpx88803e212Qcq5c+ecbNNYgD8zLpxdZ3dItW72gQMHvOts0YiwzdaIG05wdl3tUG7q7dsAvgKgv7TKBAC9qtofuHYAaMn3RkJIbTCosYvIIgA9qrqplAeIyBIR2SgiG0t5PyEkHYoZjb8dwKdE5AEAowBcAeA7AMaLyIikd28F0Jnvzaq6DMAyoLoz6AiJnYuaLisi8wH8raouEpEfA/ipqi4Xkf8CsFVV/2OQ96davCIsDNHW1ubkBQsWeG023l6zZo2TN2zY4F1nU3ZhzG7bbLFIO80V8OPysKhkjDCez5ZKTJf9KoAvishu5GL475ZxL0JIhbmoSTWq+gqAVxJ5L4Bb0leJEFIJMp9BV8zMrWJdu5MnT3rHdqWYTa+F19pVZOEsNqtfqGuhWmqhq07XtDAD/U4Z8lQWzo0nJBJo7IREQtVq0IVYPQbSybp9YWEIW1wibLNu/ZkzZ4pTmAxIKSWiOTJfeVi8gpDIobETEgk0dkIigds/DUEKjXsAxY99pAHj79qEMTshkUNjJyQSMp9BN1zcuyx3Jh0zZox3bFOMthYe4KcV7eKfSmA/d7GzDUn1YM9OSCTQ2AmJBBo7IZEwpOvGV7pWeZjiss+zhSzC62y9+XA/N/u+UF8bf9u4fMKECQX1sjXqAeDQoUNOtnXuK72iLPwshVYPXmSxlJLeR/LDnp2QSKCxExIJQ86NT2PbIutah9sy2zRXmPKyK+msq25dbgBobGx0st12Knxe+FmsG29d8PD+hw8fRiGOHDlSsC1L0nC76bqnC3t2QiKBxk5IJAw5N95yMW7e2LFjnWxHt8Ny1Pa60AW3o+ADufENDQ1Oti494LvxYYENO2Jud3sNt4zq6upycjhLzrr/dIOJhT07IZFAYyckEmjshETCkIvZB4pDbUrNxs0AcO211zq5ubnZydOmTfOuszF7fX2912ZjbKtHuPLMptSK3UIqPLZbQh89etS7zh6HtfNtDJ9GzM4CFcOHooxdRPYBOAngPIA+VW0TkUYAzwOYAmAfgEdU9Vhl1CSElMvFuPELVHW2qvbvnvgUgHWqOgPAuuSYEFKjlOPGfxrA/ER+Drk94L5apj5l0dLS4uS5c+d6bXfddZeTbUotdNVtffnQhbVt1n22aTLAX5wSLlSxbvZ7771XsM3eM9wl1s6SC9NyaResoKs+fCi2Z1cAvxKRTSKyJDk3SVX7E77dACalrh0hJDWK7dnvUNVOEbkSwBoR2WkbVVVFJG8XkPxzWJKvjRCSHUX17Krambz2APgZcls1HxKRZgBIXnsKvHeZqraZWJ8QUgUG7dlFZCyAS1T1ZCLfB+AfAawC8DiAp5PXlZVUtBhsGu3OO+/02hYuXOhkm+IKY94DBw44+dgxP7lgU142Fj948KB3nY237dRWwE+phc+2U13t+IA9DxReHQdw22NSmGLc+EkAfpYMVo0A8D+q+gsReR3ACyKyGMB+AI9UTk1CSLkMauyquhfArDznjwK4uxJKEULSJ/MZdP3prDRSOmHhialTp+aVAd/d3bt3r5P37dvnXbd9+3Ynh258T8+FYQmbDrN13wA/pXb8+PGC+hOSJZwbT0gk0NgJiQQaOyGRMOT2erMrz5qamrw2W0lm//79Xtu2bducvGXLFieH8XZnZ6eTz54967XZ+NvuqWZlIJ2imMMJrpyrDdizExIJNHZCImFIFK+wrrut5R4WqLDptfb2dq/NHm/evNnJ4cqz06dPl6cs6JqG8PdRG7BnJyQSaOyERMKQcONtbblx48bllQF/9Ly7u9tr27nzwqrcgbZPImS4wp6dkEigsRMSCTR2QiJhSMTsdi81u9ItjNltscVwxVp4TEhssGcnJBJo7IREwpBz423N97B4hV0IEy5isbXfSPpwsUvtw56dkEigsRMSCTR2QiKhJmN2u61xeGy3VLYy4Mfptu468MH66uVSaoGKWoxf04i3w+vsPa1ci58/FtizExIJNHZCIqEm3fgRI3y1rBtoi1eEqTe79VGYekub4eyOFnLrh/NnjoGienYRGS8iPxGRnSKyQ0TmiUijiKwRkV3Ja8PgdyKEVIti3fjvAPiFqn4Eua2gdgB4CsA6VZ0BYF1yTAipUYrZxbUewJ0AngAAVT0H4JyIfBrA/OSy5wC8AuCraShlZ8KFx9aNt0UtAH8hTFhbjhRmIPe81JF6uvy1RzE9+1QAhwH8t4i8KSLPJls3T1LV/v2Iu5Hb7ZUQUqMUY+wjAMwF8J+qOgfAaQQuu+b+jef9Vy4iS0Rko4hsLFdZQkjpFGPsHQA6VHVDcvwT5Iz/kIg0A0Dy2pPvzaq6TFXbVLUtDYUJIaVRzP7s3SJyUESuU9V25PZk3578PA7g6eR1ZVpKhbPdbPxnV8BZGQD6+vqcfOrUqaKeFcak9jgcOyg0K2wgwtjVHof3sDMFbfpx1KhR3nW2jn6oo00/2jEM+7sB/FWAYVvasw1JbVBsnv1vAPxQREYC2AvgSeS8ghdEZDGA/QAeqYyKhJA0KMrYVXUzgHxu+N3pqkMIqRQ1OYPOuqKAn2KzqbfQDbZbNxXrxluXGPDd53BBjr02fJ/F6h9+loGebd11+5nDba7s7yB08e3zTp48mVcGgJ6eC0MsYZrSuvFMoQ0fODeekEigsRMSCTR2QiKhZmJ2m0IK40Qbo1o5jIdtXBqmkyw21g/jZpvOC6fj2mvDeN5iY95QD/vs8P6jR4/OKzc1NXnX1dfXOzlc+WefffToUSeHKwmPHz/u5DRWCIbjJ4z1aw/27IREAo2dkEiQLN0tETmM3ASciQCOZPbg/NSCDgD1CKEePherxzWq2pSvIVNjdw8V2VjtufK1oAP1oB5Z6kE3npBIoLETEgnVMvZlVXqupRZ0AKhHCPXwSU2PqsTshJDsoRtPSCRkauwicr+ItIvIbhHJrBqtiHxPRHpE5C1zLvNS2CIyWUReFpHtIrJNRJZWQxcRGSUivxeRLYke30jOTxWRDcn383xSv6DiiEhdUt9wdbX0EJF9IvIHEdncX0KtSn8jFSvbnpmxi0gdgH8H8BcAZgJ4VERmZvT47wO4PzhXjVLYfQC+pKozAdwG4PPJ7yBrXc4CWKiqswDMBnC/iNwG4JsAvqWq0wEcA7C4wnr0sxS58uT9VEuPBao626S6qvE3Urmy7aqayQ+AeQB+aY6/BuBrGT5/CoC3zHE7gOZEbgbQnpUuRoeVAO6tpi4AxgB4A8CtyE3eGJHv+6rg81uTP+CFAFYDkCrpsQ/AxOBcpt8LgHoA7yAZS0tbjyzd+BYAB81xR3KuWlS1FLaITAEwB8CGauiSuM6bkSsUugbAHgC9qtq/cier7+fbAL4CoH9V04Qq6aEAfiUim0RkSXIu6++lomXbOUCHgUthVwIRGQfgpwC+oKonqqGLqp5X1dnI9ay3APhIpZ8ZIiKLAPSo6qasn52HO1R1LnJh5udF5E7bmNH3UlbZ9sHI0tg7AUw2x63JuWpRVCnstBGRS5Ez9B+q6opq6gIAqtoL4GXk3OXxItK/FjaL7+d2AJ8SkX0AliPnyn+nCnpAVTuT1x4AP0PuH2DW30tZZdsHI0tjfx3AjGSkdSSAzwJYleHzQ1YhVwIbSLkUdiEkt+j7uwB2qOq/VksXEWkSkfGJPBq5cYMdyBn9w1npoapfU9VWVZ2C3N/D/6nqY1nrISJjReTyfhnAfQDeQsbfi6p2AzgoItclp/rLtqejR6UHPoKBhgcAvI1cfPj3GT73RwC6ALyP3H/PxcjFhusA7AKwFkBjBnrcgZwLthXA5uTngax1AXATgDcTPd4C8A/J+WkAfg9gN4AfA7gsw+9oPoDV1dAjed6W5Gdb/99mlf5GZgPYmHw3/wugIS09OIOOkEjgAB0hkUBjJyQSaOyERAKNnZBIoLETEgk0dkIigcZOSCTQ2AmJhP8H6i59mnXulSkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for i in range(len(Y_train_orig[0])):\n",
    "#     if Y_train_orig[0][i]==2:\n",
    "#         print(i)\n",
    "index = 82\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 800\n",
      "number of test examples = 200\n",
      "X_train shape: (800, 64, 64, 3)\n",
      "Y_train shape: (800, 60)\n",
      "X_test shape: (200, 64, 64, 3)\n",
      "Y_test shape: (200, 60)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "Y_train = convert_to_one_hot(Y_train_orig, len(classes)).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, len(classes)).T\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "conv_layers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_H0 -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(dtype=tf.float32,shape=[None, n_H0, n_W0, n_C0])\n",
    "    Y = tf.placeholder(dtype=tf.float32,shape=[None, n_y])\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]\n",
    "                        W2 : [2, 2, 8, 16]\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1) \n",
    "        \n",
    "    W1 = tf.get_variable(\"W1\", [4,4,3,8], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W2 = tf.get_variable(\"W2\", [2,2,8,16], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    # CONV2D: stride of 1, padding 'SAME'\n",
    "    Z1 = tf.nn.conv2d(X,W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    # MAXPOOL: window 8x8, sride 8, padding 'SAME'\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n",
    "    # FLATTEN\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n",
    "    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n",
    "    Z3 = tf.contrib.layers.fully_connected(P2, len(classes), activation_fn=None)\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n",
    "          num_epochs = 50, minibatch_size = 8, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer ConvNet in Tensorflow:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    seed = 3                                          # to keep results consistent (numpy seed)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]            \n",
    "\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                _ , temp_cost = sess.run([optimizer,cost],feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "        \n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-fd37c176019c>:13: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Cost after epoch 0: 4.077286\n",
      "Cost after epoch 5: 1.774581\n",
      "Cost after epoch 10: 0.888071\n",
      "Cost after epoch 15: 0.486790\n",
      "Cost after epoch 20: 0.280457\n"
     ]
    }
   ],
   "source": [
    "_, _, parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
